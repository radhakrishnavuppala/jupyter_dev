{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Trial\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def say_hello(to):\n",
    "    \"Say Hello to someone\"\n",
    "    return f'Hello {to}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Krishna!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"Krishna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# from pyspark.sql.functions import regexp_extract, concat, lit\n",
    "# from pyspark.sql.functions import col, to_date, to_timestamp, when, date_format\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "# Compute the time in hours between the given columns. \n",
    "# Used for computing the time in hours between two timestamps. \n",
    "def computeDayHours(col1, col2):\n",
    "    '''Compute the time in hours between the given columns. \n",
    "       Used for computing the time in hours between two timestamps.'''\n",
    "    return (to_timestamp(col1).cast(LongType()) - to_timestamp(col2).cast(LongType()))/3600\n",
    "\n",
    "\n",
    "\n",
    "def computeDayHoursRest(col1, col2):\n",
    "    '''Used to compute the time from first time stamp and in the day given by the second.\n",
    "       Example: time duration between 5.45 upto midnight. We pass the 5.45 and the day in the col2. \n",
    "       timestamp of just day gives us time at 00 hrs, which is the day start. \n",
    "       So we compute 24 - [col1_time - day_start]'''\n",
    "    return 24 - (to_timestamp(col1).cast(LongType()) - to_timestamp(col2).cast(LongType()))/3600\n",
    "\n",
    "\n",
    "\n",
    "def loadData(spark, dataPath, bootstrap=True):\n",
    "    '''loads the data into the DF''' \n",
    "    csvFile = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"sep\", \"\\t\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(dataPath)\\\n",
    "    .coalesce(8)\\\n",
    "    .select('dK','yM','sT','eT','hr','hMo','mMB','mb','mo','vcId')\n",
    "    \n",
    "    # This conf setting is required to allow the below queries to work correctly. \n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "    csvFile = csvFile.withColumn('um', regexp_extract(col('dK'), 'umId_(.*)vcId.*', 1))\\\n",
    "                .withColumn('vm', concat(lit(\"vm-\"), regexp_extract(col('dK'), '.*vm-(.*)', 1)))\n",
    "    \n",
    "    return csvFile\n",
    "\n",
    "def generatePerDayStats(dataFrame):\n",
    "    ''' This functions takes an interval based dataframe and computes per day statistics '''\n",
    "    dataFrameByDay = dataFrame.withColumn(\"startDate\", to_date(col(\"sT\"), \"yyyy-MM-d\"))\\\n",
    "                    .withColumn(\"endDate\", to_date(col(\"eT\"), \"yyyy-MM-d\"))\\\n",
    "                    .withColumn(\"curDate\", F.explode(F.expr(\"sequence(startDate, endDate, interval 1 day)\")))\\\n",
    "                    .withColumn(\"yearMonth\", date_format(col(\"curDate\"), 'yyyyMM'))\\\n",
    "                    .withColumn(\"dayDiff\", computeDayHours(col('eT'), col('sT')))\\\n",
    "                    .withColumn(\"firstPartial\", computeDayHoursRest(col('sT'), col('curDate')))\\\n",
    "                    .withColumn(\"endPartial\", computeDayHours(col('eT'), col('curDate')))\\\n",
    "                    .withColumn(\"dayHours\", \n",
    "                                when(col(\"startDate\") == col(\"endDate\"), col(\"dayDiff\"))\n",
    "                                .when(col(\"startDate\") == col(\"curDate\"), col(\"firstPartial\"))\n",
    "                                .when(col(\"endDate\") == col(\"curDate\"), col(\"endPartial\"))\n",
    "                                .otherwise(24))\\\n",
    "                    .withColumn('mbhrs', F.expr(\"dayHours * mb\"))\\\n",
    "                    .drop('dayDiff','firstPartial','endPartial', 'dK')\n",
    "\n",
    "    \n",
    "    dataFrameDaySum = dataFrameByDay.groupBy(col(\"um\"), col(\"curDate\"))\\\n",
    "             .agg(F.collect_set('hMo').alias('numHost'), F.collect_set('vm').alias('numVms'), \\\n",
    "                  F.sum('dayHours').alias('dayHours'), F.sum('mb').alias('mb'), \\\n",
    "                  F.sum('mbhrs').alias('mbhrs'))\n",
    "    \n",
    "    w = (Window()\n",
    "         .partitionBy(col(\"um\"))\n",
    "         .orderBy(F.col(\"curDate\"))\n",
    "         .rowsBetween(-7, 0))\n",
    "\n",
    "    csvFileDaySumRolling = dataFrameDaySum\\\n",
    "             .withColumn('dayHours_avg', F.avg(\"dayHours\").over(w))\\\n",
    "             .withColumn('mbhrs_avg', F.avg(\"mbhrs\").over(w)) \\\n",
    "             .withColumn('mb_avg', F.avg(\"mb\").over(w))\\\n",
    "             .withColumn('numVms_avg', F.avg(\"numVms\").over(w))\\\n",
    "             .withColumn('numHost_avg', F.avg(\"numHost\").over(w))\n",
    "\n",
    "    return csvFileDaySumRolling\n",
    "\n",
    "\n",
    "def generateUMStats(dataFrame):\n",
    "    ''' This functions takes an interval based dataframe and computes UM statistics '''\n",
    "\n",
    "    csvFileByDay = dataFrame.withColumn(\"startDate\", to_date(col(\"sT\"), \"yyyy-MM-d\"))\\\n",
    "                        .withColumn(\"endDate\", to_date(col(\"eT\"), \"yyyy-MM-d\"))\\\n",
    "                        .withColumn(\"curDate\", F.explode(F.expr(\"sequence(startDate, endDate, interval 1 day)\")))\\\n",
    "                        .withColumn(\"yearMonth\", date_format(col(\"curDate\"), 'yyyyMM'))\\\n",
    "                        .withColumn(\"dayDiff\", computeDayHours(col('eT'), col('sT')))\\\n",
    "                        .withColumn(\"firstPartial\", computeDayHoursRest(col('sT'), col('curDate')))\\\n",
    "                        .withColumn(\"endPartial\", computeDayHours(col('eT'), col('curDate')))\\\n",
    "                        .withColumn(\"dayHours\", \n",
    "                                    when(col(\"startDate\") == col(\"endDate\"), col(\"dayDiff\"))\n",
    "                                    .when(col(\"startDate\") == col(\"curDate\"), col(\"firstPartial\"))\n",
    "                                    .when(col(\"endDate\") == col(\"curDate\"), col(\"endPartial\"))\n",
    "                                    .otherwise(24))\\\n",
    "                        .withColumn('mbhrs', F.expr(\"dayHours * mb\"))\\\n",
    "                        .drop('dayDiff','firstPartial','endPartial', 'dK')\n",
    "    \n",
    "    csvFileDaySum = csvFileByDay.groupBy(col(\"um\"), col(\"yearMonth\"))\\\n",
    "                 .agg(F.collect_set('hMo').alias('hostCount'), F.collect_set('vm').alias('vmCount'), \\\n",
    "                      F.sum('dayHours').alias('duration'), F.sum('mb').alias('mbCount'), \\\n",
    "                      F.approx_count_distinct('sT', 0.05).alias('recCount'),\\\n",
    "                      F.sum('mbhrs').alias('mbhrs'))\n",
    "\n",
    "    return csvFileDaySum\n",
    "\n",
    "def writeData(dataFrame, filename):\n",
    "    df = dataFrame.toPandas()\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "#from pyspark import SparkConf\n",
    "def getSpark():\n",
    "    '''Create a spark context'''\n",
    "    appName = \"UM stats handling\"\n",
    "    master = \"local\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(appName)\\\n",
    "        .config(\"spark.executor.instances\", \"6\")\\\n",
    "        .config(\"spark.executor.cores\", \"4\")\\\n",
    "        .config(\"spark.kubernetes.executor.request.cpu\", \"4\")\\\n",
    "        .config(\"spark.kubernetes.executor.limit.cpu\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"5g\")\\\n",
    "        .config(\"spark.kubernetes.executor.request.memory\", \"5G\")\\\n",
    "        .config(\"spark.kubernetes.executor.limit.memory\", \"5G\")\\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
